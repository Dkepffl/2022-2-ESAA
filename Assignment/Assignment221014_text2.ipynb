{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP6nb25QevJAM5ai9+1g1Dq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dkepffl/2022-2-ESAA/blob/main/Assignment/Assignment221014_text2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CHAPTER 08 텍스트 분석**\n",
        "___\n"
      ],
      "metadata": {
        "id": "ZlMi1iUj8v9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import packages\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "vMtT2l_hAaNS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **04 텍스트 분류 실습-20 뉴스그룹 분류**\n",
        "___\n",
        "- 사이킷런이 내부 예제 데이터인 20 뉴스그룹 데이터셋을 이용해 텍스트 분류를 적용해보자.\n",
        "- 텍스트 분류는 특정 문서의 분류를 학습 데이터를 통해 학습해 모델을 생성한 뒤 이 학습 모델을 이용해 다른 문서의 분류를 예측하는 것이다.\n",
        "- 텍스트를 기반으로 분류를 수행할 때는 먼저 텍스트를 정규화한 뒤 피처 벡터화를 적용한다. 그리고 적합한 머신러닝 알고리즘을 적용해 분류를 학습/예측/평가합니다.\n",
        "  - 텍스트를 피처 벡터화로 변환하면 일반적으로 희소 행렬 형태가 되는데, 로지스틱 회귀, 선형 서포트 벡터 머신, 나이브 베이즈 등이 이러한 형태의 데이터를 잘 분류한다. 이 중 로지스틱 회귀를 이용해 분류를 수행할 예정이다.\n",
        "  - 카운트 기반과 TF-IDF 기반 벡터화를 적용한다.\n",
        "\n"
      ],
      "metadata": {
        "id": "dkEcaWz_c7XM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### | **텍스트 정규화**\n",
        "- `fetch_20newgroups()`를 이용하여 데이터를 로딩한다.\n",
        "\n"
      ],
      "metadata": {
        "id": "i_aiiAk7AncI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "news_data = fetch_20newsgroups(subset='all',random_state=156)"
      ],
      "metadata": {
        "id": "mX-7I6Zk_nLW"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `fetch_20newgroups()`는 사이킷런의 다른 데이터셋 예제처럼 파이썬 딕셔너리와 유사한 Bunch 객체를 반환한다. 어떠한 key 값을 가지고 있는지 확인해보자."
      ],
      "metadata": {
        "id": "76pTwMA3_uzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(news_data.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESgeotGSACz-",
        "outputId": "de9c5f64-62e0-41aa-f78d-4b1ba4ea43a6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 다음으로 Target클래스가 어떻게 구성되어 있는지 확인해보자."
      ],
      "metadata": {
        "id": "bW-KU9akANae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('target 클래스의 값과 분포도 \\n',pd.Series(news_data.target).value_counts().sort_index())\n",
        "print('target 클래스의 이름들 \\n',news_data.target_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gs_6BtB4AYPh",
        "outputId": "6c0ee955-46bd-4d6c-f7e8-e75cb17157a1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "target 클래스의 값과 분포도 \n",
            " 0     799\n",
            "1     973\n",
            "2     985\n",
            "3     982\n",
            "4     963\n",
            "5     988\n",
            "6     975\n",
            "7     990\n",
            "8     996\n",
            "9     994\n",
            "10    999\n",
            "11    991\n",
            "12    984\n",
            "13    990\n",
            "14    987\n",
            "15    997\n",
            "16    910\n",
            "17    940\n",
            "18    775\n",
            "19    628\n",
            "dtype: int64\n",
            "target 클래스의 이름들 \n",
            " ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Target 클래스의 값은 0부터 19까지 20개로 구성되어 있으며, 위의 출력 결과처럼 주어져 있다.\n",
        "- 개별 데이터가 텍스트로 어떻게 구성되어 있는지 데이터를 한 개만 추출해 값을 확인해보자."
      ],
      "metadata": {
        "id": "D5IeEsy6AqBg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(news_data.data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sizv9WcrA1HD",
        "outputId": "162a05f3-2cfe-4ca3-a698-9ae50f5c484d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "From: egreen@east.sun.com (Ed Green - Pixel Cruncher)\n",
            "Subject: Re: Observation re: helmets\n",
            "Organization: Sun Microsystems, RTP, NC\n",
            "Lines: 21\n",
            "Distribution: world\n",
            "Reply-To: egreen@east.sun.com\n",
            "NNTP-Posting-Host: laser.east.sun.com\n",
            "\n",
            "In article 211353@mavenry.altcit.eskimo.com, maven@mavenry.altcit.eskimo.com (Norman Hamer) writes:\n",
            "> \n",
            "> The question for the day is re: passenger helmets, if you don't know for \n",
            ">certain who's gonna ride with you (like say you meet them at a .... church \n",
            ">meeting, yeah, that's the ticket)... What are some guidelines? Should I just \n",
            ">pick up another shoei in my size to have a backup helmet (XL), or should I \n",
            ">maybe get an inexpensive one of a smaller size to accomodate my likely \n",
            ">passenger? \n",
            "\n",
            "If your primary concern is protecting the passenger in the event of a\n",
            "crash, have him or her fitted for a helmet that is their size.  If your\n",
            "primary concern is complying with stupid helmet laws, carry a real big\n",
            "spare (you can put a big or small head in a big helmet, but not in a\n",
            "small one).\n",
            "\n",
            "---\n",
            "Ed Green, former Ninjaite |I was drinking last night with a biker,\n",
            "  Ed.Green@East.Sun.COM   |and I showed him a picture of you.  I said,\n",
            "DoD #0111  (919)460-8302  |\"Go on, get to know her, you'll like her!\"\n",
            " (The Grateful Dead) -->  |It seemed like the least I could do...\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 텍스트 데이터를 확인해보면, 뉴스그룹 기사의 내용뿐만 아니라 제목, 작성자, 소속, 이메일 등의 다양한 정보를 가지고 있다.\n",
        "- 제목과 소속, 이메일 주소 등의 헤더와 푸터 정보들은 Target 클래스 값과 유사한 데이터를 가지고 있는 경우가 많아, 대부분의 ML 알고리즘에서 높은 성능을 보인다. 여기서는 순수한 텍스트만으로 구성된 내용을 제외한 다른 정보는 제거한다.\n",
        "  + `remove` 파라미터를 이용하여 헤더(header), 푸터(footer) 등을 제거할 수 있다.\n",
        "  + `fetch_20newsgroups()`는 subset 파라미터를 이용해 학습 데이터셋과 테스트 데이터셋을 분리해 저장할 수 있다.\n"
      ],
      "metadata": {
        "id": "LcJ4r5-9B5TC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# subset='train'으로 학습용(Train) 데이터만 추출, remove=('headers','footers','quotes')로 내용만 추출\n",
        "train_news= fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), random_state=156)\n",
        "X_train = train_news.data\n",
        "y_train = train_news.target\n",
        "\n",
        "# subset='test'으로 테스트(Test) 데이터만 추출, remove=('headers', 'footers', 'quotes')로 내용만 추출\n",
        "test_news= fetch_20newsgroups(subset='test',remove=('headers', 'footers','quotes'),random_state=156)\n",
        "X_test = test_news.data\n",
        "y_test = test_news.target\n",
        "print('학습 데이터 크기 {0} , 테스트 데이터 크기 {1}'.format(len(train_news.data) , len(test_news.data)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2-NfORdDNub",
        "outputId": "d59c3fd4-c7d2-466b-8278-58e19c255d7e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "학습 데이터 크기 11314 , 테스트 데이터 크기 7532\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### | **피처 벡터화 변환과 머신러닝 모델 학습/예측/평가**\n",
        "- 학습 데이터는 11314개의 뉴스그룹 문서가 리스트 형태로 주어지고, 테스트 데이터는 7532개의 문서가 역시 리스트 형태로 주어졌다.\n",
        "- `CountVectorizer`를 이용해 학습 데이터의 텍스트를 피처 벡터화하자.\n",
        "- 테스트 데이터 역시 피처 벡터화를 수행하는데, 테스트 데이터에서 `CountVectorizer`를 적용할 때 반드시 학습 데이터를 이용해 `fit()`이 수행된 객체를 이용하여 테스트 데이터를 변환(transform)해야 한다.\n",
        "  + 그래야 학습 시 설정된 `CountVectorizer`의 피처 개수와 테스트 데이터의 피처 개수가 같아지기 때문이다.\n",
        "  + 테스트 데이터의 피처 벡터화는 학습 데이터에 사용된 `CountCextorizer` 객체 변수인 `cnt_vect.transform()`을 이용해 변환한다.\n",
        "  + 같은 이유로 테스트 데이터의 피처 벡터화 시 `fit_transform()`을 사용하면 안된다.\n"
      ],
      "metadata": {
        "id": "h3mrcLkCXNfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Count Vectorization으로 feature extraction 변환 수행. \n",
        "cnt_vect = CountVectorizer()\n",
        "cnt_vect.fit(X_train)\n",
        "X_train_cnt_vect = cnt_vect.transform(X_train)\n",
        "\n",
        "# 학습 데이터로 fit( )된 CountVectorizer를 이용하여 테스트 데이터를 feature extraction 변환 수행. \n",
        "X_test_cnt_vect = cnt_vect.transform(X_test)\n",
        "\n",
        "print('학습 데이터 Text의 CountVectorizer Shape:',X_train_cnt_vect.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pl2ZiWJsFLJe",
        "outputId": "da54fc3b-2f17-4c58-bd5b-4e72cd38368a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "학습 데이터 Text의 CountVectorizer Shape: (11314, 101631)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 학습 데이터를 `CountVectorizer`로 피처 추출한 결과, 11314개의 문서에서 피처, 즉 단어가 101631개 만들어졌다.\n",
        "- 이렇게 피처 벡터화된 데이터에 로지스틱 회귀를 적용해 뉴스 그룹에 대한 분류를 예측해보자."
      ],
      "metadata": {
        "id": "U2B5lJj3FVRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# LogisticRegression을 이용하여 학습/예측/평가 수행. \n",
        "lr_clf = LogisticRegression()\n",
        "lr_clf.fit(X_train_cnt_vect , y_train)\n",
        "pred = lr_clf.predict(X_test_cnt_vect)\n",
        "\n",
        "print('CountVectorized Logistic Regression 의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test,pred)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8oi1VieFhpJ",
        "outputId": "bd9ef224-eb9b-4453-9c43-40d2430028d1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CountVectorized Logistic Regression 의 예측 정확도는 0.608\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Count 기반으로 피처 벡터화가 적용된 데이터셋에 대한 로지스틱 회귀 예측 정확도는 약 0.608이다.\n",
        "- 이번에는 TF-IDF 기반으로 벡터화를 하여 예측 모델을 수행해보자."
      ],
      "metadata": {
        "id": "q6AjSUCnFlko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# TF-IDF Vectorization 적용하여 학습 데이터셋과 테스트 데이터 셋 변환. \n",
        "tfidf_vect = TfidfVectorizer()\n",
        "tfidf_vect.fit(X_train)\n",
        "X_train_tfidf_vect = tfidf_vect.transform(X_train)\n",
        "X_test_tfidf_vect = tfidf_vect.transform(X_test)"
      ],
      "metadata": {
        "id": "fIBEDfzXFukQ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LogisticRegression을 이용하여 학습/예측/평가 수행. \n",
        "lr_clf = LogisticRegression()\n",
        "lr_clf.fit(X_train_tfidf_vect , y_train)\n",
        "pred = lr_clf.predict(X_test_tfidf_vect)\n",
        "print('TF-IDF Logistic Regression 의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test ,pred)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U04ZyEvxFwxp",
        "outputId": "b80b3dc4-6f8c-4592-e9c1-0f027858b97c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Logistic Regression 의 예측 정확도는 0.674\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- TF-IDF가 단순 카운트 기반보다 훨씬 높은 예측 정확도를 제공한다. 일반적으로 문서 내에 텍스트가 많고 문서 개수가 많은 텍스트 분석에서는 TF-IDF 벡터화가 Count 벡터화보다 좋은 예측 결과를 도출한다.\n",
        "- 텍스트 분석에서 머신러닝 모델의 성능을 향상시키는 중요한 2가지 방법은 최적의 ML 알고리즘을 선택하는 것과 최상의 피처 전처리를 수행하는 것이다.\n",
        "- 텍스트 정규화나 Count/TF-IDF 기반 피처 벡터화를 어떻게 효과적으로 적용했는지가 텍스트 기반의 머신러닝 성능에 큰 영향을 미칠 수 있다.\n",
        "- 이번에는 좀 더 다양한 파라미터를 적용하여 TF-IDF 벡터화를 해보자.\n",
        "  + 스톱 워드를 `'None'`에서 `'english'`로 변경한다.\n",
        "  + `ngram_range`를 기존 (1,1)에서 (1,2)로 변경한다.\n",
        "  + `max_df`는 300으로 변경한다."
      ],
      "metadata": {
        "id": "ri35vHLnFymY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# stop words 필터링을 추가하고 ngram을 기본(1,1)에서 (1,2)로 변경하여 Feature Vectorization 적용\n",
        "tfidf_vect = TfidfVectorizer(stop_words='english', ngram_range=(1,2), max_df=300 )\n",
        "tfidf_vect.fit(X_train)\n",
        "X_train_tfidf_vect = tfidf_vect.transform(X_train)\n",
        "X_test_tfidf_vect = tfidf_vect.transform(X_test)\n",
        "\n",
        "lr_clf = LogisticRegression()\n",
        "lr_clf.fit(X_train_tfidf_vect , y_train)\n",
        "pred = lr_clf.predict(X_test_tfidf_vect)\n",
        "print('TF-IDF Vectorized Logistic Regression 의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test ,pred)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkXvZphMGgx4",
        "outputId": "1a9e26fb-6b00-4a2d-effa-eaf54e063b8d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Vectorized Logistic Regression 의 예측 정확도는 0.692\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 이번에는 GridSearchCV를 이용해 로지스틱 회귀의 하이퍼 파라미터 최적화를 수행해보자. 로지스틱 회귀의 C 파라미터만 변경해서 최적의 C값을 찾은 뒤 이 값으로 학습된 모델에서 테스트 데이터로 예측해 모델 성능을 평가한다."
      ],
      "metadata": {
        "id": "BJAYRRRBGoZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# 최적 C 값 도출 튜닝 수행. CV는 3 Fold셋으로 설정. \n",
        "params = { 'C':[0.01, 0.1, 1, 5, 10]}\n",
        "grid_cv_lr = GridSearchCV(lr_clf ,param_grid=params , cv=3 , scoring='accuracy' , verbose=1 )\n",
        "grid_cv_lr.fit(X_train_tfidf_vect , y_train)\n",
        "print('Logistic Regression best C parameter :',grid_cv_lr.best_params_ )\n",
        "\n",
        "# 최적 C 값으로 학습된 grid_cv로 예측 수행하고 정확도 평가. \n",
        "pred = grid_cv_lr.predict(X_test_tfidf_vect)\n",
        "print('TF-IDF Vectorized Logistic Regression 의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test ,pred)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_vDBz40Gzgl",
        "outputId": "915ae3d5-8585-44e3-860c-090bceb24593"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression best C parameter : {'C': 10}\n",
            "TF-IDF Vectorized Logistic Regression 의 예측 정확도는 0.701\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 로지스틱 회귀의 C가 10일 때 GridSearchCV의 교차 검증 테스트셋에서 가장 좋은 예측 성능을 보였으며, 이를 테스트 데이터셋에 적용하면, 약 0.703으로 이전보다 약간 향상된 성능을 보여준다."
      ],
      "metadata": {
        "id": "gy2nJwyBG3k-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### | **사이킷런 파이프라인(Pipeline) 사용 및 GridSearchCV와의 결합**\n",
        "- 사이킷런의 `Pipeline` 클래스를 이용하면 피처 벡터화와 ML 알고리즘 학습/예측을 위한 코드 작성을 한 번에 진행할 수 있다.\n",
        "- 일반적으로 머신러닝에서 Pipeline이란 데이터의 가공, 변환 등의 전처리와 알고리즘 적용을 마치 **'수도관(Pipe)'에서 물이 흐르듯' 한꺼번에 스트림 기반으로 처리**한다는 의미이다.\n",
        "- `Pipiline` 사용의 장점\n",
        "  - 이렇게 `Pipeline`을 이용하면 데이터의 전처리와 머신러닝 학습 과정을 통일된 API 기반에서 처리할 수 있어 더 직관적인 ML 모델 코드를 생성할 수 있다.\n",
        "  - 또한 대용량 데이터의 피처 벡터화 결과를 별도의 데이터로 저장하지 않고, 스트림 기반에서 바로 머신러닝 알고리즘의 데이터로 입력할 수 있어 수행 시간을 절약할 수 있다.\n",
        "- 일반적으로 사이킷런의 `Pipeline`은 텍스트 기반의 피처 벡터화뿐만 아니라 모든 데이터 전처리 작업과 Estimator를 결합할 수 있다.\n",
        "  + 예를 들어 스케일링 또는 벡터 정규화, PCA 등의 변환 작업과 분류, 회귀 등의 Estimator를 한 번에 결합\n",
        "- 다음은 위에서 텍스트 분류 예제 코드를 `Pipeline`을 이용해 다시 작성한 코드이다.\n",
        "```python\n",
        "pipeline = Pipeline([('tfidf_vect', TfidVectorizer(stop_words='english')),\n",
        "                        ('lr_clf', LogisticRegression(random_state=156))])\n",
        "```\n",
        "  + `TfidVectorizer` 객체를 `tfidf_vect`라는 객체 변수 명으로, `LogisticRegression` 객체를 `lr_clf`라는 객체 변수명으로 생성한다.\n",
        "  + 그리고 두 객체를 파이프라인으로 연결하는 `Pipeline` 객체 `pipeline`을 생성한다.\n",
        "- 아래의 코드는 피처 벡터화와 머신러닝 모델의 학습과 예측이 Pipeline의 `fit()`과 `transform()`으로 통일되어 수행한다."
      ],
      "metadata": {
        "id": "Sdl9WdeAHFL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# TfidfVectorizer 객체를 tfidf_vect 객체명으로, LogisticRegression객체를 lr_clf 객체명으로 생성하는 Pipeline생성\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf_vect', TfidfVectorizer(stop_words='english', ngram_range=(1,2), max_df=300)),\n",
        "    ('lr_clf', LogisticRegression(C=10))\n",
        "])\n",
        "\n",
        "# 별도의 TfidfVectorizer객체의 fit_transform( )과 LogisticRegression의 fit(), predict( )가 필요 없음 \n",
        "# pipeline의 fit() 과 predict() 만으로 한꺼번에 Feature Vectorization과 ML 학습/예측이 가능\n",
        "pipeline.fit(X_train, y_train)\n",
        "pred = pipeline.predict(X_test)\n",
        "\n",
        "print('Pipeline을 통한 Logistic Regression 의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test ,pred)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPcTJYGfHMJl",
        "outputId": "f5134805-978b-4ec6-c3ba-874942ee3035"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline을 통한 Logistic Regression 의 예측 정확도는 0.701\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 사이킷런은 `GridSearchCV` 클래스의 생성 파라미터로 `Pipeline`을 입력하여, `Pipeline` 기반에서도 하이퍼 파라미터 튜닝을 그리드서치 방식으로 진행할 수 있게 지원한다. 이렇게 하면 피처 벡터화를 위한 파라미터와 ML 알고리즘의 하이퍼 파라미터를 한 번에 `GridSearchCV`를 이용하여 최적화할 수 있다.\n",
        "- 다음 예제에서는 `GridSearchCV`에 `Pipeline`을 입력하여 `TfidfVectorizer`의 파라미터와 `LogisticRegression`의 하이퍼파라미터를 함께 최적화한다.\n",
        "  + `GridSearchCV`에 Estimator가 아닌 Pipeline을 입력할 경우, `param_grid`의 입력 값 설정이 하이퍼 파라미터명이 객체 변수명과 결합되어 제공된다는 점에서 기존과 조금 다르다. \n",
        "- `GridSearchCV`에 `Pipeline`을 적용할 때 유의할 점은 모든 파라미터를 최적화하면 시간이 많이 걸린다는 점이다."
      ],
      "metadata": {
        "id": "Vsx2nbxlJ63a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf_vect', TfidfVectorizer(stop_words='english')),\n",
        "    ('lr_clf', LogisticRegression())\n",
        "])\n",
        "\n",
        "# Pipeline에 기술된 각각의 객체 변수에 언더바(_)2개를 연달아 붙여 GridSearchCV에 사용될 \n",
        "# 파라미터/하이퍼 파라미터 이름과 값을 설정. . \n",
        "params = { 'tfidf_vect__ngram_range': [(1,1), (1,2), (1,3)],\n",
        "           'tfidf_vect__max_df': [100, 300, 700],\n",
        "           'lr_clf__C': [1,5,10]\n",
        "}\n",
        "\n",
        "# GridSearchCV의 생성자에 Estimator가 아닌 Pipeline 객체 입력\n",
        "grid_cv_pipe = GridSearchCV(pipeline, param_grid=params, cv=3 , scoring='accuracy',verbose=1)\n",
        "grid_cv_pipe.fit(X_train , y_train)\n",
        "print(grid_cv_pipe.best_params_ , grid_cv_pipe.best_score_)\n",
        "\n",
        "pred = grid_cv_pipe.predict(X_test)\n",
        "print('Pipeline을 통한 Logistic Regression 의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test ,pred)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jc2x8QjfHNNd",
        "outputId": "6d892b14-bcd1-4714-de09-3e04319b5dfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 로지스틱 회귀 외에 서포트 벡터머신과 나이브 베이즈 알고리즘도 희소 행렬 기반의 텍스트 분류에서 자주 사용되는 머신러닝 알고리즘이다."
      ],
      "metadata": {
        "id": "vvQ9JtxELnb8"
      }
    }
  ]
}