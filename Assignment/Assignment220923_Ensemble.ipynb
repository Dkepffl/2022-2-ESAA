{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNIsbz3z74CO3AVL7pFmhbu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dkepffl/2022-2-ESAA/blob/main/Assignment/Assignment220923_Ensemble.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import package\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)"
      ],
      "metadata": {
        "id": "f5n2m077bgII"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "78nhk3rjaHnH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CHAPTER 7 앙상블 학습과 랜덤 포레스트**\n",
        "___\n",
        "- 일련의 예측 모델(즉, 분류나 회귀 모델)로부터 예측을 수집하면 가장 좋은 모델 하나보다 더 좋은 예측을 얻을 수 있을 수 있다. 이 일련의 예측기를 **앙상블**이라고 하며, 이 모델을 학습 시키는 것을 **앙상블 학습(ensemble learning)**이라고 부른다. 또한 앙상블 학습 알고리즘을 **앙상블 방법(ensemble method)**라고 한다.\n",
        "- 앙상블 방법 중 한 가지로 훈련 세트로부터 무작위로 각기 다른 서브셋을 만들어 일련의 결정 트리 분류기를 훈련시킨 다음, 모든 개별 트리의 예측을 구해 가장 많은 선택을 받은 클래스를 예측으로 삼는 결정 트리의 앙상블 **랜덤 포레스트(random forest)**라고 한다.\n",
        "- 이 장에서는 **배깅, 부스팅, 스태킹** 등 가장 인기 있는 앙상블 방법을 설명한다."
      ],
      "metadata": {
        "id": "ZlMi1iUj8v9G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7.1 투표 기반 분류기**\n",
        "____\n",
        "- 정확도가 80%인 분류 모델 여러 개(예 : 로지스틱 회귀 분류 모델, SVM 분류 모델, 랜덤 포레스트 분류 모델, KNN 분류 모델 등)를 훈련시켰다고 가정하자. \n",
        "- 더 좋은 분류 모델을 만드는 매우 간단한 방법은 각 분류 모델의 예측을 모아, 가장 많이 선택된 클래스를 예측하는 것이다. 이렇게 다수결 투표로 정해지는 분류 모델을 **직접 투표(hand voting)** 분류 모델이라고 한다.\n",
        "  - 이 다수결 투표 분류 모델이 앙상블에 포함된 개별 분류 모델 중 가장 뛰어난 것보다 정확도가 높을 수 있다. \n",
        "  - 또한 각 분류 모델이 **약한 학습기(weak learner, 랜덤 추측보다 조금 더 높은 성능을 내는 분류 모델)**이라도 충분히 많고 다양하다면 앙상블은 **강한 학습기(strong learner, 높은 정확도를 가진 분류 모델)**라고 한다.\n",
        "- 앙상블 모델은 각 분류 모델이 독립적이고 오차에 상관 관계가 적을수록 좋은 성능을 낸다. \n",
        "  + 같은 데이터로 훈련시키기 때문에 완벽하게 독립적이고 오차에 상관 관계가 없을 수는 없다.\n",
        "  + 따라서 각기 다른 알고리즘으로 학습시켜야 매우 다른 종류의 오차를 만들 가능성이 높아, 앙상블 모델의 정확도를 향상시킨다.\n",
        "- 다음은 여러 분류 모델을 조합하여 사이킷런의 투표 기반 분류기(`VotingClassifier`)를 만들고 훈련시키는 코드이다."
      ],
      "metadata": {
        "id": "UQbvEqH48xhX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import data\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_moons\n",
        "\n",
        "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
      ],
      "metadata": {
        "id": "jhcqAuZ9gVmW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# 각 분류 모델 학습\n",
        "log_clf = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
        "rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "svm_clf = SVC(gamma=\"scale\", random_state=42)"
      ],
      "metadata": {
        "id": "lh2kNXK9ftmj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 투표 기반 분류기 생성\n",
        "voting_clf = VotingClassifier(estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)], \n",
        "                              voting='hard')\n",
        "\n",
        "# 분류 모델 학습\n",
        "voting_clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNUu5hZyf9e4",
        "outputId": "00b153da-85d4-467a-9cf8-f7bca485113f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VotingClassifier(estimators=[('lr', LogisticRegression(random_state=42)),\n",
              "                             ('rf', RandomForestClassifier(random_state=42)),\n",
              "                             ('svc', SVC(random_state=42))])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 각 분류 모델과 투표 기반 분로 모델의 테스트셋에서의 정확도를 확인해보자."
      ],
      "metadata": {
        "id": "RXy6Ii0IgEdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDTczMIIgHg8",
        "outputId": "46b138ab-04f3-44fb-c873-dd503db8ac63"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogisticRegression 0.864\n",
            "RandomForestClassifier 0.896\n",
            "SVC 0.896\n",
            "VotingClassifier 0.912\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 투표 기반 분류 모델이 다른 개별 분류 모델보다 성능이 조금 더 높다.\n",
        "- 모든 분류 모델가 클래스의 확률을 예측할 수 있으면(즉, `predict_proba()` 메서드가 있으면) 개별 분류 모델의 예측을 평균 내어 확률이 가장 높은 클래스를 예측할 수 있다. 이를 **간접 투표(soft voting)**라고 한다.\n",
        "  + 확률이 높은 투표에 비중을 더 두어 직접 투표 방식보다 성능이 높다.\n",
        "  + 사이킷런의 `VotingClassifier()` 클래스에서 이 방식을 사용하려면, `voting='hard'`를 `voting='soft'`로 바꾸고 모든 분류 모델의 클래스 확률을 추정할 수 있으면 된다.\n",
        "  + `SVC`는 기본값에서는 클래스 확률을 제공하지 않으므로, `probability` 매개변수를 `True`로 지정해야 한다."
      ],
      "metadata": {
        "id": "MEewmhfSgedt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7.2 배깅과 페이스팅**\n",
        "___\n",
        "- 앞서 말했듯이 다양한 분류기를 만드는 한 가지 방법은 각기 다른 훈련 알고리즘을 사용하는 것이다. 또 다른 방법은 같은 알고리즘을 사용하고 훈련 세트의 서브셋을 무작위로 구성하여 분류기를 각기 다르게 학습시키는 것이다.\n",
        "  + 훈련 세트에서 중복을 허용하여 샘플링하는 방식을 **배깅(bagging)**이라고 한다.\n",
        "  + 중복을 허용하지 않고 샘플링하는 방식을 **페이스팅(pasting)**이라고 한다.\n",
        "- 모든 예측 모델이 훈련을 마치면, 앙상블은 모든 예측 모델의 예측을 모아서 새로운 샘플에 대한 예측을 만든다. 수집 함수는 **분류일 때 보통 통계적 최빈값**(직접 투표 분류 모델처럼 가장 많은 예측 결과)이고 **회귀에 대해서는 평균**을 계산한다.\n",
        "  + 개별 예측 모델은 원본 훈련 세트로 훈련시킨 것보다 훨씬 크게 편향되어 있지만 수집 함수를 통과하면 편향과 분산이 모두 감소한다.\n",
        "  + 일반적으로 앙상블의 결과는 원본 데이터셋으로 하나의 예측기를 훈련시킬 때와 비교해 편향과 비슷하지만 분산은 줄어든다."
      ],
      "metadata": {
        "id": "HmaED9viV0DW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### | **7.2.1 사이킷런의 배깅과 페이스팅**\n",
        "- 사이킷런은 배깅과 페이스팅을 위해 간편한 API로 구성된 `BaggingClassifier`(회귀의 경우, `BaggingRegressor`)를 제공한다.\n",
        "- 다음은 결정 트리 분류 모델 500개의 앙상블을 훈련시키는 코드이다.\n",
        "  + 각 분류 모델은 훈련 세트에서 중복을 허용하여 무작위로 선택된 100개의 샘플로 훈련된다. 즉, 배깅 방식이다. 페이스팅을 사용하려면 `bootstrap=False`로 설정하면 도니다.\n",
        "  + `n_jobs` 매개변수는 사이킷런이 훈련과 예측에 사용할 CPU 코어 수를 결정한다.\n"
      ],
      "metadata": {
        "id": "BvtS1jwl80Ef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500,\n",
        "    max_samples=100, bootstrap=True, random_state=42)\n",
        "\n",
        "bag_clf.fit(X_train, y_train)\n",
        "y_pred = bag_clf.predict(X_test)"
      ],
      "metadata": {
        "id": "crFg37XR_QTq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 부트스트래핑은 각 예측기가 학습하는 서브셋에 다양성을 증가시키므로, 배깅이 페이스팅보다 편향이 조금 더 높다.\n",
        "- 하지만 다양성을 추가한다는 것은 예측 모델 사이의 상관 관계를 줄이므로, 앙상블의 분산을 감소시킨다.\n",
        "- 전반적으로 배깅이 더 나은 모델을 만들기 때문에 일반적으로 배깅을 더 선호한다. 하지만 시간과 CPU 파워에 여유가 있다면 교차 검증으로 배깅과 페이스팅을 모두 평가해 더 나은 쪽을 선택하는 것이 좋다."
      ],
      "metadata": {
        "id": "9Pk7oK9f_dza"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### | **7.2.2 oob 평가**\n",
        "- 배깅을 사용하면 어떤 샘플은 한 예측 모델을 위해 여러 번 샘플링 되고, 어떤 샘플은 전혀 선택되지 않을 수 있다.\n",
        "- `BaggingClassifier`는 기본적으로 중복을 허용하여(Default : `boootstrap=True`) 훈련 세트의 크기(=m) 만큼 샘플을 선택한다. 이는 평균적으로 각 예측 모델에 훈련 샘플의 63% 정도만 샘플링된다는 것을 의미한다. \n",
        "  + 이때 선택되지 않은 훈련 샘플의 나머지 37%를 oob 샘플(out-of-bag)이라고 부른다.\n",
        "  + 예측마다 남겨진 oob 샘플은 모두 다르다.\n",
        "- 예측 모델이 훈련되는 동안 oob 샘플을 사용하지 않으므로, 별도의 검증 세트를 사용하지 않고 oob 샘플을 사용해 평가할 수 있다. 앙상블 모델의 평가는 각 예측 모델의 oob 평가를 평균하여 얻는다.\n",
        "- 사이킷런에서 `BaggingClassifier`를 만들 때, `oob_score=True`라고 설정하면, 훈련이 끝난 후 자동으로 oob 평가를 수행하고 결과를 `oob_score_` 변수에 저장한다.\n"
      ],
      "metadata": {
        "id": "i_aiiAk7AncI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bag_clf = BaggingClassifier(\n",
        "    DecisionTreeClassifier(), n_estimators=500,\n",
        "    bootstrap=True, oob_score=True, random_state=40)\n",
        "bag_clf.fit(X_train, y_train)\n",
        "\n",
        "bag_clf.oob_score_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3w83pSAAvfV",
        "outputId": "a04f6821-4bd8-4cf3-dd39-ccccad9803a4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8986666666666666"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- oob 평가 결과를 보면 이 `BaggingClassifier`는 테스트셋에서 약 89.9%의 정확도를 얻을 것으로 보인다."
      ],
      "metadata": {
        "id": "L5SykcvwAz5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 실제 테스트셋에서의 정확도\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "y_pred = bag_clf.predict(X_test)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YONe6iD8A_09",
        "outputId": "43c38e4a-77b0-4747-d515-6b80261e7f87"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.912"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 테스트셋에서 91.2%의 정확도를 얻었다.\n",
        "- oob 샘플에 대한 결정 함수의 값도 `oob_decision_function` 변수에서 확인할 수 있다. 이 경우 기반이 되는 예측 모델이 `predict_proba()` 메서드를 가지고 있어 결정 함수는 각 훈련 샘플의 클래스 확률을 반환한다."
      ],
      "metadata": {
        "id": "KOJKLmYPBG2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bag_clf.oob_decision_function_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Q5w4U-2BY-K",
        "outputId": "7b64820b-a941-4470-be58-a0103c79215a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.32275132, 0.67724868],\n",
              "       [0.34117647, 0.65882353],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.09497207, 0.90502793],\n",
              "       [0.31147541, 0.68852459],\n",
              "       [0.01754386, 0.98245614],\n",
              "       [0.97109827, 0.02890173],\n",
              "       [0.97765363, 0.02234637],\n",
              "       [0.74404762, 0.25595238],\n",
              "       [0.        , 1.        ],\n",
              "       [0.7173913 , 0.2826087 ],\n",
              "       [0.85026738, 0.14973262],\n",
              "       [0.97222222, 0.02777778],\n",
              "       [0.0625    , 0.9375    ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.97837838, 0.02162162],\n",
              "       [0.94642857, 0.05357143],\n",
              "       [1.        , 0.        ],\n",
              "       [0.01704545, 0.98295455],\n",
              "       [0.39473684, 0.60526316],\n",
              "       [0.88700565, 0.11299435],\n",
              "       [1.        , 0.        ],\n",
              "       [0.97790055, 0.02209945],\n",
              "       [0.        , 1.        ],\n",
              "       [0.99428571, 0.00571429],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.62569832, 0.37430168],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.13402062, 0.86597938],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.38251366, 0.61748634],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.27093596, 0.72906404],\n",
              "       [0.34146341, 0.65853659],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.00531915, 0.99468085],\n",
              "       [0.98843931, 0.01156069],\n",
              "       [0.91428571, 0.08571429],\n",
              "       [0.97282609, 0.02717391],\n",
              "       [0.98019802, 0.01980198],\n",
              "       [0.        , 1.        ],\n",
              "       [0.07361963, 0.92638037],\n",
              "       [0.98019802, 0.01980198],\n",
              "       [0.0052356 , 0.9947644 ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.97790055, 0.02209945],\n",
              "       [0.8       , 0.2       ],\n",
              "       [0.42424242, 0.57575758],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.66477273, 0.33522727],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.86781609, 0.13218391],\n",
              "       [1.        , 0.        ],\n",
              "       [0.56725146, 0.43274854],\n",
              "       [0.1576087 , 0.8423913 ],\n",
              "       [0.66492147, 0.33507853],\n",
              "       [0.91709845, 0.08290155],\n",
              "       [0.        , 1.        ],\n",
              "       [0.16759777, 0.83240223],\n",
              "       [0.87434555, 0.12565445],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.995     , 0.005     ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.07878788, 0.92121212],\n",
              "       [0.05418719, 0.94581281],\n",
              "       [0.29015544, 0.70984456],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.83040936, 0.16959064],\n",
              "       [0.01092896, 0.98907104],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.21465969, 0.78534031],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.94660194, 0.05339806],\n",
              "       [0.77094972, 0.22905028],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.16574586, 0.83425414],\n",
              "       [0.65306122, 0.34693878],\n",
              "       [0.        , 1.        ],\n",
              "       [0.02564103, 0.97435897],\n",
              "       [0.50555556, 0.49444444],\n",
              "       [1.        , 0.        ],\n",
              "       [0.03208556, 0.96791444],\n",
              "       [0.99435028, 0.00564972],\n",
              "       [0.23699422, 0.76300578],\n",
              "       [0.49509804, 0.50490196],\n",
              "       [0.9947644 , 0.0052356 ],\n",
              "       [0.00555556, 0.99444444],\n",
              "       [0.98963731, 0.01036269],\n",
              "       [0.26153846, 0.73846154],\n",
              "       [0.92972973, 0.07027027],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.80113636, 0.19886364],\n",
              "       [1.        , 0.        ],\n",
              "       [0.0106383 , 0.9893617 ],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.98181818, 0.01818182],\n",
              "       [1.        , 0.        ],\n",
              "       [0.01036269, 0.98963731],\n",
              "       [0.97752809, 0.02247191],\n",
              "       [0.99453552, 0.00546448],\n",
              "       [0.01960784, 0.98039216],\n",
              "       [0.17857143, 0.82142857],\n",
              "       [0.98387097, 0.01612903],\n",
              "       [0.29533679, 0.70466321],\n",
              "       [0.98295455, 0.01704545],\n",
              "       [0.        , 1.        ],\n",
              "       [0.00561798, 0.99438202],\n",
              "       [0.75690608, 0.24309392],\n",
              "       [0.38624339, 0.61375661],\n",
              "       [0.40625   , 0.59375   ],\n",
              "       [0.87368421, 0.12631579],\n",
              "       [0.92462312, 0.07537688],\n",
              "       [0.05181347, 0.94818653],\n",
              "       [0.82802548, 0.17197452],\n",
              "       [0.01546392, 0.98453608],\n",
              "       [0.        , 1.        ],\n",
              "       [0.02298851, 0.97701149],\n",
              "       [0.9726776 , 0.0273224 ],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.01041667, 0.98958333],\n",
              "       [0.        , 1.        ],\n",
              "       [0.03804348, 0.96195652],\n",
              "       [0.02040816, 0.97959184],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.94915254, 0.05084746],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.99462366, 0.00537634],\n",
              "       [0.        , 1.        ],\n",
              "       [0.39378238, 0.60621762],\n",
              "       [0.33152174, 0.66847826],\n",
              "       [0.00609756, 0.99390244],\n",
              "       [0.        , 1.        ],\n",
              "       [0.3172043 , 0.6827957 ],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.00588235, 0.99411765],\n",
              "       [0.        , 1.        ],\n",
              "       [0.98924731, 0.01075269],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.62893082, 0.37106918],\n",
              "       [0.92344498, 0.07655502],\n",
              "       [0.        , 1.        ],\n",
              "       [0.99526066, 0.00473934],\n",
              "       [1.        , 0.        ],\n",
              "       [0.98888889, 0.01111111],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.06989247, 0.93010753],\n",
              "       [1.        , 0.        ],\n",
              "       [0.03608247, 0.96391753],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.02185792, 0.97814208],\n",
              "       [1.        , 0.        ],\n",
              "       [0.95808383, 0.04191617],\n",
              "       [0.78362573, 0.21637427],\n",
              "       [0.56650246, 0.43349754],\n",
              "       [0.        , 1.        ],\n",
              "       [0.18023256, 0.81976744],\n",
              "       [1.        , 0.        ],\n",
              "       [0.93121693, 0.06878307],\n",
              "       [0.97175141, 0.02824859],\n",
              "       [1.        , 0.        ],\n",
              "       [0.00531915, 0.99468085],\n",
              "       [0.        , 1.        ],\n",
              "       [0.43010753, 0.56989247],\n",
              "       [0.85858586, 0.14141414],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.00558659, 0.99441341],\n",
              "       [0.        , 1.        ],\n",
              "       [0.96923077, 0.03076923],\n",
              "       [0.        , 1.        ],\n",
              "       [0.21649485, 0.78350515],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.98477157, 0.01522843],\n",
              "       [0.8       , 0.2       ],\n",
              "       [0.99441341, 0.00558659],\n",
              "       [0.        , 1.        ],\n",
              "       [0.09497207, 0.90502793],\n",
              "       [0.99492386, 0.00507614],\n",
              "       [0.01714286, 0.98285714],\n",
              "       [0.        , 1.        ],\n",
              "       [0.02747253, 0.97252747],\n",
              "       [1.        , 0.        ],\n",
              "       [0.77005348, 0.22994652],\n",
              "       [0.        , 1.        ],\n",
              "       [0.90229885, 0.09770115],\n",
              "       [0.98387097, 0.01612903],\n",
              "       [0.22222222, 0.77777778],\n",
              "       [0.20348837, 0.79651163],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.20338983, 0.79661017],\n",
              "       [0.98181818, 0.01818182],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.98969072, 0.01030928],\n",
              "       [0.        , 1.        ],\n",
              "       [0.48663102, 0.51336898],\n",
              "       [1.        , 0.        ],\n",
              "       [0.00529101, 0.99470899],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.08379888, 0.91620112],\n",
              "       [0.12352941, 0.87647059],\n",
              "       [0.99415205, 0.00584795],\n",
              "       [0.03517588, 0.96482412],\n",
              "       [1.        , 0.        ],\n",
              "       [0.39790576, 0.60209424],\n",
              "       [0.05434783, 0.94565217],\n",
              "       [0.53191489, 0.46808511],\n",
              "       [0.51898734, 0.48101266],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.60869565, 0.39130435],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.24157303, 0.75842697],\n",
              "       [0.81578947, 0.18421053],\n",
              "       [0.08717949, 0.91282051],\n",
              "       [0.99453552, 0.00546448],\n",
              "       [0.82142857, 0.17857143],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.11904762, 0.88095238],\n",
              "       [0.04188482, 0.95811518],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.89150943, 0.10849057],\n",
              "       [0.19230769, 0.80769231],\n",
              "       [0.95238095, 0.04761905],\n",
              "       [0.00515464, 0.99484536],\n",
              "       [0.59375   , 0.40625   ],\n",
              "       [0.07692308, 0.92307692],\n",
              "       [0.99484536, 0.00515464],\n",
              "       [0.83684211, 0.16315789],\n",
              "       [0.        , 1.        ],\n",
              "       [0.99484536, 0.00515464],\n",
              "       [0.95360825, 0.04639175],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.26395939, 0.73604061],\n",
              "       [0.98461538, 0.01538462],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.00574713, 0.99425287],\n",
              "       [0.85142857, 0.14857143],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.75301205, 0.24698795],\n",
              "       [0.8969697 , 0.1030303 ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.75555556, 0.24444444],\n",
              "       [0.48863636, 0.51136364],\n",
              "       [0.        , 1.        ],\n",
              "       [0.92473118, 0.07526882],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.87709497, 0.12290503],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.74752475, 0.25247525],\n",
              "       [0.09146341, 0.90853659],\n",
              "       [0.42268041, 0.57731959],\n",
              "       [0.22395833, 0.77604167],\n",
              "       [0.        , 1.        ],\n",
              "       [0.87046632, 0.12953368],\n",
              "       [0.78212291, 0.21787709],\n",
              "       [0.00507614, 0.99492386],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.02884615, 0.97115385],\n",
              "       [0.96      , 0.04      ],\n",
              "       [0.93478261, 0.06521739],\n",
              "       [1.        , 0.        ],\n",
              "       [0.50731707, 0.49268293],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.01604278, 0.98395722],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.96987952, 0.03012048],\n",
              "       [0.        , 1.        ],\n",
              "       [0.05172414, 0.94827586],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.99494949, 0.00505051],\n",
              "       [0.01675978, 0.98324022],\n",
              "       [1.        , 0.        ],\n",
              "       [0.14583333, 0.85416667],\n",
              "       [0.        , 1.        ],\n",
              "       [0.00546448, 0.99453552],\n",
              "       [0.        , 1.        ],\n",
              "       [0.41836735, 0.58163265],\n",
              "       [0.13095238, 0.86904762],\n",
              "       [0.22110553, 0.77889447],\n",
              "       [1.        , 0.        ],\n",
              "       [0.97647059, 0.02352941],\n",
              "       [0.21195652, 0.78804348],\n",
              "       [0.98882682, 0.01117318],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.96428571, 0.03571429],\n",
              "       [0.34554974, 0.65445026],\n",
              "       [0.98235294, 0.01764706],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.99465241, 0.00534759],\n",
              "       [0.        , 1.        ],\n",
              "       [0.06043956, 0.93956044],\n",
              "       [0.98214286, 0.01785714],\n",
              "       [1.        , 0.        ],\n",
              "       [0.03108808, 0.96891192],\n",
              "       [0.58854167, 0.41145833]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7.3 랜덤 패치와 랜덤 서브스페이스**\n",
        "____\n",
        "- `BaggingClassifier`는 피처 샘플링도 지원한다. 샘플링은 `max_features`, `bootstrap_features` 두 매개변수로 조절된다.\n",
        "  + 작동 방식은 `max_samples`, `bootstrap`과 동일하지만 샘플이 아니고 피처에 대한 샘플링이다.\n",
        "  + 각 예측 모델은 무작위로 선택한 입력 피처의 일부분으로 훈련된다.\n",
        "  + 이 기법은 이미지와 같은 매우 고차원의 데이터셋을 다룰 때 유용하다.\n",
        "- 훈련 피처와 샘플을 모두 샘플링하는 것을 **랜덤 패치 방식(random patches method)**이라고 한다.\n",
        "- 훈련 샘플을 모두 사용하고 피처는 샘플링하는 것은 **랜덤 서브스페이스 방식(random subspaces method)**이라고 한다.\n",
        "- 피처 샘플링은 더 다양한 예측 모델을 만들어 편향을 늘리는 대신 분산을 줄인다.\n"
      ],
      "metadata": {
        "id": "KqjepJmhYbBF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7.4 랜덤 포레스트**\n",
        "____\n",
        "- 랜덤 포레스트는 일반적으로 배깅 방법(또는 페이스팅)을 적용한 결정 트리의 앙상블이다.\n",
        "  + `max_samples`로 훈련 세트의 크기를 지정한다.\n",
        "  + 결정 트리에 최적화되어 사용하기 편리한 `RandomForestClassifier`를 사용할 수 있다.\n",
        "  + 회귀 문제를 위해서는 `RandomForestRegressor`를 사용한다."
      ],
      "metadata": {
        "id": "cRASnv79Yedy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# 500개의 트리로 이루어진 랜덤 포레스트 모델\n",
        "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, random_state=42)\n",
        "rnd_clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred_rf = rnd_clf.predict(X_test)"
      ],
      "metadata": {
        "id": "meEGlHvtCtFu"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 랜덤 포레스트 알고리즘은 트리의 노드를 분할할 때 전체 피처 중에서 최선의 피처를 찾는 대신 무작위로 선택한 피처 후부 중에서 최적의 피처를 찾는 식으로 무작위성을 더 주입한다.\n",
        "- 이는 결국 트리를 더욱 다양하게 만들고 편향을 늘리는 대신 분산을 낮추어 전체적으로 더 훌륭한 모델을 만들어낸다.\n",
        "- 다음은 `BaggingClassifier`를 사용해 앞의 `RandomForestClassifier`와 거의 유사하게 만든 것이다."
      ],
      "metadata": {
        "id": "Nb-CwXOOC0PP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BaggingClassifier를 사용해 만든 랜덤 포레스트 모델\n",
        "bag_clf = BaggingClassifier(\n",
        "    DecisionTreeClassifier(max_features=\"sqrt\", max_leaf_nodes=16),\n",
        "    n_estimators=500, random_state=42)\n",
        "\n",
        "bag_clf.fit(X_train, y_train)\n",
        "y_pred = bag_clf.predict(X_test)"
      ],
      "metadata": {
        "id": "Ek8sB5rHDO0w"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 예측 값 비교\n",
        "np.sum(y_pred == y_pred_rf) / len(y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaG7-4x9DWKq",
        "outputId": "48b3c601-45e4-4969-e403-35382616ebfa"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### | **7.4.1 엑스트라 트리**\n",
        "- 랜덤 포레스트에서 트리를 만들 때 각 노드는 무작위로 피처의 서브셋을 만들어 분할에 사용한다.\n",
        "- 트리를 더욱 무작위하게 만들기 위해 최적의 임계값을 찾는 대신 후보 피처를 사용해 무작위로 분할한 다음 그중에서 최상의 분할을 선택한다.\n",
        "- 이와 같이 극단적으로 무작위한 트리의 랜덤 포레스트를 **익스트림 랜덤 트리(extremely randomized trees)** 앙상불 혹은 줄여서 엑스트라 트리라고 부른다.\n",
        "  + 여기서도 편향이 늘어나지만 분산이 줄어든다.\n",
        "  + 모든 노드에서 피처마다 가장 최적의 임계값을 찾아 트리 알고리즘 시간이 길어지는데, 엑스트라 트리는 그렇지 않기 때문에 훨씬 빠르다.\n",
        "- 엑스트라 트리는 사이킷런의 `ExtraTreesClassifier`를 사용한다."
      ],
      "metadata": {
        "id": "X_tjg6wwYjs3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### | **7.4.2 특성 중요도**\n",
        "- 랜덤 포레스트의 또 다른 장점은 피처의 상대적 중요도를 측정하기 쉽다는 것이다. \n",
        "  + 사이킷런은 어떤 피처를 사용한 노드가 평균적으로 불순도를 얼마나 감소시키는지 계산하여 피처의 중요도를 측정한다.\n",
        "  + 정확히는 가중치 평균이며, 각 노드의 가중치는 연관된 훈련 샘플 수와 같다.\n",
        "- 사이킷런은 훈련이 끝난 뒤 자동으로 이 점수를 계산하고 중요도의 전체 합이 1이 되도록 결과값을 정규화한다.\n",
        "  + 이 값은 `feature_importances_` 변수에 저장되어 있다."
      ],
      "metadata": {
        "id": "Dm0W1F8FYp9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# import data\n",
        "iris = load_iris()"
      ],
      "metadata": {
        "id": "71X8zS16EqL_"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model fitting\n",
        "rnd_clf = RandomForestClassifier(n_estimators=500, random_state=42)\n",
        "rnd_clf.fit(iris[\"data\"], iris[\"target\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXtvl6jwEvjC",
        "outputId": "e3aec06f-4b17-4b61-b5ef-e6b1e3031626"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(n_estimators=500, random_state=42)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print the importances of features\n",
        "for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n",
        "    print(name, score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhNMeSJ1E18U",
        "outputId": "08f8baab-262b-424a-a746-ef7ffc2d5a10"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sepal length (cm) 0.11249225099876375\n",
            "sepal width (cm) 0.02311928828251033\n",
            "petal length (cm) 0.4410304643639577\n",
            "petal width (cm) 0.4233579963547682\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7.5 부스팅**\n",
        "____\n",
        "- 부스팅(boosting)은 약한 학습기를 여러 개 연결하여 강한 학습기를 만드는 앙상블 방법을 말한다. 이때 앞의 모델을 보완하면서 일련의 예측 모델을 학습시킨다.\n",
        "- 부스팅 방법에는 여러 가지가 있지만 가장 인기있는 것은 에이다부스트와 그레디언트 부스팅이다."
      ],
      "metadata": {
        "id": "wsmUwcV-Yusr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### | **7.5.1 에이다부스트**\n",
        "- 이전 예측기를 보완하는 새로운 예측기를 만드는 방법은 이전 모델이 과소적합했던 훈련 샘플의 가중치를 더 높이는 것이다. 이렇게 하면 새로운 예측기는 학습하기 어려운 샘플에 점점 더 맞춰지게 된다. 이것이 **에이다부스트**에서 사용하는 방식이다.\n",
        "  + 분류 모델을 훈련 세트에서 훈련시키고 예측을 만든 다음 잘못 분류된 훈련 샘플의 가중치를 상대적으로 높이는 것을 반복한다.\n",
        "- 모든 예측 모델의 훈련이 끝나면, 이 앙상블 모델은 배깅이나 페이스팅과 비슷한 방식으로 예측을 만든다. 하지만 가중치가 적용된 훈련 세트의 전반적인 정확도에 따라 예측 모델마다 다른 가중치가 적용된다.\n",
        "- 에이다부스트 훈련 알고리즘\n",
        "  1. 각 샘플에 대한 가중치 $w^{(i)}$를 $\\frac{1}{m}$으로 초기화하고, 첫 번째 모델을 학습한다.\n",
        "  2. 가중치가 적용된 에러율 $r_1$을 훈련 세트에 대해 계산한다.\n",
        "  $$r_j = \\frac{\\sum_{i=1, \\hat{y_j} \\ne y^{(i)}}^{m}w^{(i)}}{\\sum_{i=1}^{m} w^{(i)}}$$\n",
        "  3. 그리고 예측 모델의 가중치 $\\alpha_j$를 계산한다. 여기서 $\\eta$는 학습률 하이퍼 파라미터이다. 예측 모델이 정확할수록 가중치가 더 높아진다. \n",
        "  $$\\alpha_j= \\eta log{\\frac{1-r_j}{r_j}}$$\n",
        "  4. 아래 식을 이용하여 샘플의 가중치를 업데이트한다. 즉, 잘못 분류된 샘플의 가중치가 증가한다.$i = 1, 2, 3, \\cdots$\n",
        "  $$ w^{(i)} = \n",
        "  \\begin{cases}\n",
        "  w^{(i)}, & \\mbox{if }\\hat{y_j} = y^{(i)} \\\\\n",
        "  w^{(i)} exp(\\alpha_j), & \\mbox{if }\\hat{y_j} \\ne y^{(i)}\n",
        "  \\end{cases} $$\n",
        "  5. 그리고 모든 샘플의 가중치를 정규화한다.\n",
        "  6. 마지막으로 새 예측 모델이 업데이트된 가중치를 사용해 훈련되고, 전체 과정을 반복하다 지정된 예측 모델의 수에 도달하거나, 완벽한 예측 모델이 만들어지면 중지된다.\n",
        "- 에이다부스트의 예측은 모든 예측 모델의 예측을 계산하고, 예측 모델의 가중치 $\\alpha_j$를 더해 예측 결과를 만든다. 가중치 합이 가장 큰 클래스가 예측 결과가 된다.\n",
        "- 사이킷런은 SAMME라는 에이다부스트의 다중 클래스 버전을 사용한다. 클래스가 두 개이면 에이다부스트와 동일하다.\n",
        "  + 예측 모델에 `predict_proba()` 메서드가 있다면 SAMME.R을 사용하여 예측값 대신 클래스 확률에 기반하여 성능이 더 좋은 모델을 만들 수 있다.\n",
        "- 다음 코드는 `AdaBoostClassifier`를 사용하여 200개의 아주 얕은 결정 트리를 기반으로 하는 에이다 부스트 분류 모델 훈련 과정이다."
      ],
      "metadata": {
        "id": "TtyuqKPjYyjt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "ada_clf = AdaBoostClassifier(\n",
        "    DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
        "    algorithm=\"SAMME.R\", learning_rate=0.5, random_state=42)\n",
        "ada_clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hl5BGPXXP3do",
        "outputId": "dd21279b-608b-4eb9-b14e-4e1f53ab2f37"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
              "                   learning_rate=0.5, n_estimators=200, random_state=42)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### | **7.5.2 그레디언트 부스팅**\n",
        "- **그레디언트 부스팅(gradient boosting)** 역시 에이다부스트처럼 앙상블에 이전까지의 오차를 보정하도록 예측 모델을 순차적으로 추가한다. 하지만 에이다부스트처럼 반복마다 샘플의 가중치를 수정하는 대신 이전 예측기가 만든 잔차(residual error)에 새로운 예측 모델을 학습시킨다.\n",
        "- 결정 트리 기반인 예측 모델을 사용하여 간단한 회귀 문제를 풀어보자. 이를 그레디언트 트리 부스팅 혹은 그레디언트 부스티드 회귀 트리라고 한다. 먼저 `DecisionTreeRegressor`를 훈련 세트에 학습시켜보자."
      ],
      "metadata": {
        "id": "_MOIv9oSY2n6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 1) - 0.5\n",
        "y = 3*X[:, 0]**2 + 0.05 * np.random.randn(100)"
      ],
      "metadata": {
        "id": "cQvD2jwWRXP8"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model fitting : DecisionTreeRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "tree_reg1 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
        "tree_reg1.fit(X, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cjvJ453Rhv2",
        "outputId": "efdcdb85-09ba-4d38-97f3-c1433ed91e81"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeRegressor(max_depth=2, random_state=42)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 이제 첫번째 예측 모델에서 생긴 잔차에 대해 두 번째 `DecisionTreeRegressor`를 훈련시킨다."
      ],
      "metadata": {
        "id": "zeqnnCayRnCc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y2 = y - tree_reg1.predict(X) # 잔차\n",
        "\n",
        "# 두 번째 DecisionTreeRegressor 학습\n",
        "tree_reg2 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
        "tree_reg2.fit(X, y2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-C0tdb4ARuAc",
        "outputId": "403954e6-ef62-4574-c609-1b4d4316c78b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeRegressor(max_depth=2, random_state=42)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 그런 다음 두 번째 예측 모델이 만든 잔차에 세 번째 회귀 모델을 훈련시킨다."
      ],
      "metadata": {
        "id": "dDD3P_v2R0cN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y3 = y - tree_reg2.predict(X) # 잔차\n",
        "\n",
        "# 세 번째 DecisionTreeRegressor 학습\n",
        "tree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
        "tree_reg3.fit(X, y3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLBEm-C6R6Gz",
        "outputId": "c8405849-545b-498b-9ad2-6d273aad8b6e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeRegressor(max_depth=2, random_state=42)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 이제 세 개의 트리를 포함하는 앙상블 모델이 생겼다. 새로운 샘플에 대한 예측을 만들려면 모든 트리의 예측을 더하면 된다."
      ],
      "metadata": {
        "id": "4Pncs4cRSDuk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트셋 생성\n",
        "X_new = np.array([[0.8]])"
      ],
      "metadata": {
        "id": "eIyfGKtWSQzY"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))"
      ],
      "metadata": {
        "id": "XMB8HKaZSKW6"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 사이킷런의 `GradientBoostingRegressor`를 사용하면 GBRT 앙상블을 간단하게 훈련시킬 수 있다."
      ],
      "metadata": {
        "id": "CLadU0FaSXlw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0, random_state=42)\n",
        "gbrt.fit(X, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ufde3ehxSfke",
        "outputId": "d839e237-5642-4ba2-83e6-4a91ce6f7ea3"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GradientBoostingRegressor(learning_rate=1.0, max_depth=2, n_estimators=3,\n",
              "                          random_state=42)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `learning_rate` 매개변수가 각 트리의 기여도를 조절한다. 위처럼 0.1로 낮게 설정하면 학습시키기 위해 많은 트리가 필요하지만 예측 성능은 좋아진다. 이 규제 방법을 축소(shrinkage)라고 한다.\n",
        "- 훈련 세트에 과대적합되는 것을 막는 최적의 트리 수를 찾기 위해서는 조기 종료 기법을 사용할 수 있다. 간단하게 `staged_predict(0` 메서드를 사용해 구현할 수 있다."
      ],
      "metadata": {
        "id": "9nOxFRojSla5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import package and split data\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=49)"
      ],
      "metadata": {
        "id": "1H_JGQYWS797"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120, random_state=42)\n",
        "gbrt.fit(X_train, y_train)\n",
        "\n",
        "# 조기 종류 기법으로 트리 수 튜닝\n",
        "errors = [mean_squared_error(y_val, y_pred)\n",
        "          for y_pred in gbrt.staged_predict(X_val)]\n",
        "bst_n_estimators = np.argmin(errors) + 1"
      ],
      "metadata": {
        "id": "qAnnVyN2TCgC"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 최적 모델 학습\n",
        "gbrt_best = GradientBoostingRegressor(max_depth=2, n_estimators=bst_n_estimators, random_state=42)\n",
        "gbrt_best.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0ikW-oVTHI3",
        "outputId": "a47863f8-e0ff-4c74-f35c-46a1fd248b3a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GradientBoostingRegressor(max_depth=2, n_estimators=56, random_state=42)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 실제로 훈련을 중지하는 방법으로 조기 종료를 구현할 수도 있다. `warm_start=True`로 설정하면, 사이킷런이 `fit()` 메서드가 호출될 때 기존 트리를 유지하면서 훈련을 추가한다.\n",
        "- 다음 코드는 연속해서 다섯 번의 반복 동안 검증 오차가 향상되지 않으면 훈련을 멈춘다."
      ],
      "metadata": {
        "id": "uzRvP73CTfDu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True, random_state=42) # 기존 트리를 유지하면서 훈련을 추가\n",
        "\n",
        "min_val_error = float(\"inf\")\n",
        "error_going_up = 0\n",
        "\n",
        "for n_estimators in range(1, 120):\n",
        "    gbrt.n_estimators = n_estimators\n",
        "\n",
        "    # model fitting and test\n",
        "    gbrt.fit(X_train, y_train)\n",
        "    y_pred = gbrt.predict(X_val)\n",
        "    val_error = mean_squared_error(y_val, y_pred)\n",
        "\n",
        "    # early stoping\n",
        "    if val_error < min_val_error:\n",
        "        min_val_error = val_error # 검증 오차 업데이트\n",
        "        error_going_up = 0\n",
        "    else:\n",
        "        error_going_up += 1 # 검증 오차가 감소하지 않는 횟수 +1\n",
        "        if error_going_up == 5:\n",
        "            break  # early stopping"
      ],
      "metadata": {
        "id": "wPJ89kAoTuK9"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `GradientBoostingRegressor`는 각 트리가 훈련할 때 사용할 훈련 샘플의 비율을 지정하는 `subsample` 매개 변수를 지원한다. 이를 이용한 방법을 확률적 그레디언트 부스팅이라고 한다.\n",
        "- XGBoost 파이썬 라이브러리를 이용하여 그레디언트 부스팅을 구현할 수도 있다."
      ],
      "metadata": {
        "id": "3-Jywd5nUGkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost\n",
        "\n",
        "xgb_reg = xgboost.XGBRegressor(random_state=42)\n",
        "xgb_reg.fit(X_train, y_train)\n",
        "y_pred = xgb_reg.predict(X_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odRmBLJdUcwZ",
        "outputId": "df9097c0-8421-4799-e60c-8006c54278ab"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[21:06:17] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 자동 조기 종료 기능\n",
        "xgb_reg.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=2)\n",
        "y_pred = xgb_reg.predict(X_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3tGegT1Uo9i",
        "outputId": "81a9c8c7-d892-4281-e4ae-0a9729e5aac6"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[21:06:48] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[0]\tvalidation_0-rmse:0.286719\n",
            "Will train until validation_0-rmse hasn't improved in 2 rounds.\n",
            "[1]\tvalidation_0-rmse:0.258221\n",
            "[2]\tvalidation_0-rmse:0.232634\n",
            "[3]\tvalidation_0-rmse:0.210526\n",
            "[4]\tvalidation_0-rmse:0.190232\n",
            "[5]\tvalidation_0-rmse:0.172196\n",
            "[6]\tvalidation_0-rmse:0.156394\n",
            "[7]\tvalidation_0-rmse:0.142241\n",
            "[8]\tvalidation_0-rmse:0.129789\n",
            "[9]\tvalidation_0-rmse:0.118752\n",
            "[10]\tvalidation_0-rmse:0.108388\n",
            "[11]\tvalidation_0-rmse:0.100155\n",
            "[12]\tvalidation_0-rmse:0.09208\n",
            "[13]\tvalidation_0-rmse:0.084791\n",
            "[14]\tvalidation_0-rmse:0.078699\n",
            "[15]\tvalidation_0-rmse:0.073248\n",
            "[16]\tvalidation_0-rmse:0.069391\n",
            "[17]\tvalidation_0-rmse:0.066277\n",
            "[18]\tvalidation_0-rmse:0.063458\n",
            "[19]\tvalidation_0-rmse:0.060326\n",
            "[20]\tvalidation_0-rmse:0.0578\n",
            "[21]\tvalidation_0-rmse:0.055643\n",
            "[22]\tvalidation_0-rmse:0.053943\n",
            "[23]\tvalidation_0-rmse:0.053138\n",
            "[24]\tvalidation_0-rmse:0.052415\n",
            "[25]\tvalidation_0-rmse:0.051821\n",
            "[26]\tvalidation_0-rmse:0.051226\n",
            "[27]\tvalidation_0-rmse:0.051135\n",
            "[28]\tvalidation_0-rmse:0.05091\n",
            "[29]\tvalidation_0-rmse:0.050893\n",
            "[30]\tvalidation_0-rmse:0.050725\n",
            "[31]\tvalidation_0-rmse:0.050471\n",
            "[32]\tvalidation_0-rmse:0.050285\n",
            "[33]\tvalidation_0-rmse:0.050492\n",
            "[34]\tvalidation_0-rmse:0.050348\n",
            "Stopping. Best iteration:\n",
            "[32]\tvalidation_0-rmse:0.050285\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7.6 스태킹**\n",
        "____\n",
        "- 스태킹(stacking)은 앙상블에 속한 모든 예측 모델의 예측을 취합하는 모델이다.\n",
        "- **메타 학습기(meta learner)** 혹은 블렌더(blender)는 각 모델의 예측을 입력 받아 최종 예측을 한다.\n",
        "- 블렌더를 학습시키는 일반적인 방법은 홀드 아웃(hold-out) 세트를 이용하는 것이다.\n",
        "  + 먼저 훈련 세트를 두 개의 서브셋으로 나눈다.\n",
        "  + 첫 번째 서브셋을 이용하여 예측 모델을 훈련시킨다.\n",
        "  + 훈련된 예측 모델로 두 번째 (홀드 아웃) 셋에 대한 예측을 만든다.\n",
        "  + 타겟값은 그대로 사용하고, 홀드 아웃 세트의 각 샘플에 대한 예측값들을 입력 피처로 사용하여 새로운 훈련 세트를 만든다.\n",
        "  + 블렌더를 새 훈련 세트로 학습시킨다."
      ],
      "metadata": {
        "id": "mXbwUNfrZAcY"
      }
    }
  ]
}